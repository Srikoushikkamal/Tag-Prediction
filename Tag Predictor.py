# -*- coding: utf-8 -*-
"""ML_Eunimartipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13NK17Trlr3OOk5IuBdbxXDCwdSsCVDTc
"""

from google.colab import drive
drive.mount('/gdrive') #Srikoushikkamal

import pandas as pd

csv=pd.read_csv('/gdrive/MyDrive/Data/Test1 - Sheet1.csv')

csv.head()

"""#Duplicates Remoed"""

csv.drop_duplicates(subset ="Title",keep = False, inplace = True)

"""#Seperate Code from body"""

New=csv['Body']

"""#Remove stop Words"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#stopwords.words('english')
rem=set(stopwords.words('english'))

New[0]

stopwords=[]
for i in New:
  z=i.split()
  strr=""
  for j in z:
    if(j not in rem):
      strr+=j
      strr+=" "
  stopwords.append(strr)
  #stopwords

"""#Remove HTML Tags"""

from bs4 import BeautifulSoup as bs

tags=[]
for text in stopwords:
  tag = bs(text,"lxml")
  tags.append(tag.get_text())
#tags

"""#Convest All to SmallCase"""

sm=[]
for i in tags:
  sm.append(i.lower())

"""#SnowBall Stem"""

import nltk
from nltk.stem.snowball import SnowballStemmer

snow = SnowballStemmer(language='english')

stem=[]
for i in sm:
  z=i.split()
  strr=""
  for j in z:
    x=snow.stem(j)
    strr+=x
    strr+=" "
  stem.append(strr)
#stem



"""#task 1 Completed

---

#ANALYSIS OF TAGS

#Total number of unique tags
"""

feq=[]
for i in csv['Tags']:
  if(i not in feq):
    feq.append(i)
len(feq)

"""#Max & Min no. of Tags Per Question"""

maxi=0
mini=0
avg=0
for i in csv["Tags"]:
  set0=len(i.split())
  if(set0>maxi):maxi=set0
  if(set0<mini):mini=set0
  avg+=set0

print("MAX Tags Per Qn.",maxi)
print("MIN Tags Per Qn.",mini)

Avg_Tag=int(avg/len(csv["Tags"]))
print("Average Tags per Qn. is", Avg_Tag)

set_dict={}
feq=[]
for i in csv['Tags']:
  z=i.split()
  for j in z:
    if(j not in feq):
      set_dict[j]=1
      feq.append(j)
    else:
      new=set_dict[j]+1
      set_dict[j]=new

for i in set_dict.keys():
  if(set_dict[i]>500):
    print(i,":",set_dict[i])

"""#Task 2 Completed

#DataSplit & Train
"""

from sklearn.model_selection import train_test_split

train1=list(set_dict.values())
y = train1
x=train1

train, test, y_train, y_test = train_test_split(x, y,test_size=0.2)

print((len(train)/len(csv["Tags"]))*100)
print((len(test)/len(csv["Tags"]))*100)

"""#Metrics in F1 Score"""

from sklearn.metrics import f1_score

#f1_score(train,#Tags, average='samples')


